{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q0tbRJr4b6d"
      },
      "source": [
        "#libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqaFcEq4v2mL",
        "outputId": "6cb7e607-04e9-4835-bee9-df7b75ba3b8e"
      },
      "source": [
        "!pip install libtiff\n",
        "!pip install scipy==1.1.0\n",
        "!pip install future\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from libtiff import TIFF\n",
        "from libtiff import TIFFfile, TIFFimage\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from scipy.misc import imresize\n",
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as keras\n",
        "from scipy.misc import imsave\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: libtiff in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TtcKEsi4nmE"
      },
      "source": [
        "#Reading the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI7NZT3IxUJd"
      },
      "source": [
        "# To read the images in numerical order\n",
        "import re\n",
        "numbers = re.compile(r'(\\d+)')\n",
        "def numericalSort(value):\n",
        "    parts = numbers.split(value)\n",
        "    parts[1::2] = map(int, parts[1::2])\n",
        "    return parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDykaC6uVabc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmq6Eto-xinu"
      },
      "source": [
        "# List of file names of actual Satellite images for traininig\n",
        "filelist_trainx = sorted(glob.glob('/content/drive/My Drive/dataset/sat/*.tif'), key=numericalSort)\n",
        "# List of file names of classified images for traininig\n",
        "filelist_trainy = sorted(glob.glob('/content/drive/My Drive/dataset/gt/*.tif'), key=numericalSort)\n",
        "\n",
        "# List of file names of actual Satellite images for testing\n",
        "filelist_testx = sorted(glob.glob('/content/drive/My Drive/dataset/sat_test/*.tif'), key=numericalSort)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zAVPdme77OW",
        "outputId": "d476a39f-a844-4f18-91d3-b14188332a52"
      },
      "source": [
        "print(filelist_testx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/dataset/sat_test/1.tif', '/content/drive/My Drive/dataset/sat_test/2.tif', '/content/drive/My Drive/dataset/sat_test/3.tif', '/content/drive/My Drive/dataset/sat_test/4.tif', '/content/drive/My Drive/dataset/sat_test/5.tif', '/content/drive/My Drive/dataset/sat_test/6.tif']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur8PEImj4uZx"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHTd2bWZ4y8I"
      },
      "source": [
        "#### Resizing the image to nearest dimensions multipls of 'stride'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edzNizFz8IWp"
      },
      "source": [
        " def resize(img, stride, n_h, n_w):\n",
        "    #h,l,_= img.shape\n",
        "    ne_h = (n_h*stride) + stride\n",
        "    ne_w = (n_w*stride) + stride\n",
        "\n",
        "    img_resized = imresize(img, (ne_h,ne_w))\n",
        "    return img_resized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1-uEg-R4200"
      },
      "source": [
        "#### Padding at the bottem and at the left of images to be able to crop them into 128*128 images for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9hnhdEv8QTg"
      },
      "source": [
        "def padding(img, w, h, c, crop_size, stride, n_h, n_w):\n",
        "\n",
        "    w_extra = w - ((n_w-1)*stride)\n",
        "    w_toadd = crop_size - w_extra\n",
        "\n",
        "    h_extra = h - ((n_h-1)*stride)\n",
        "    h_toadd = crop_size - h_extra\n",
        "\n",
        "    img_pad = np.zeros(((h+h_toadd), (w+w_toadd), c))\n",
        "    #img_pad[:h, :w,:] = img\n",
        "    #img_pad = img_pad+img\n",
        "    img_pad = np.pad(img, [(0, h_toadd), (0, w_toadd), (0,0)], mode='constant')\n",
        "\n",
        "    return img_pad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haty_cIQ47N7"
      },
      "source": [
        "#### Adding pixels to make the image with shape in multiples of stride"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eijOTe2S8ZXy"
      },
      "source": [
        "def add_pixals(img, h, w, c, n_h, n_w, crop_size, stride):\n",
        "\n",
        "    w_extra = w - ((n_w-1)*stride)\n",
        "    w_toadd = crop_size - w_extra\n",
        "\n",
        "    h_extra = h - ((n_h-1)*stride)\n",
        "    h_toadd = crop_size - h_extra\n",
        "\n",
        "    img_add = np.zeros(((h+h_toadd), (w+w_toadd), c))\n",
        "\n",
        "    img_add[:h, :w,:] = img\n",
        "    img_add[h:, :w,:] = img[:h_toadd,:, :]\n",
        "    img_add[:h,w:,:] = img[:,:w_toadd,:]\n",
        "    img_add[h:,w:,:] = img[h-h_toadd:h,w-w_toadd:w,:]\n",
        "\n",
        "    return img_add"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMTN60bi4-y1"
      },
      "source": [
        "#### Slicing the image into crop_size*crop_size crops with a stride of 32 and makking list out of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLCzCALo8fxF"
      },
      "source": [
        "def crops(a, crop_size = 128):\n",
        "\n",
        "    #stride = int(crop_size/2)\n",
        "    stride = 32\n",
        "\n",
        "    croped_images = []\n",
        "    h, w, c = a.shape\n",
        "\n",
        "    n_h = int(int(h/stride))\n",
        "    n_w = int(int(w/stride))\n",
        "\n",
        "    # Padding using the padding function we wrote\n",
        "    a = padding(a, w, h, c, crop_size, stride, n_h, n_w)\n",
        "    for i in range(n_h-1):\n",
        "        for j in range(n_w-1):\n",
        "            crop_x = a[(i*stride):((i*stride)+crop_size), (j*stride):((j*stride)+crop_size), :]\n",
        "            croped_images.append(crop_x)\n",
        "    return croped_images\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3hyNhaZ5CCv"
      },
      "source": [
        "#### Reading, padding, cropping and making array of all the cropped images of all the trainig sat images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMKqiCQ68-yx"
      },
      "source": [
        "trainx_list = []\n",
        "\n",
        "for fname in filelist_trainx[:13]:\n",
        "\n",
        "    # Reading the image\n",
        "    tif = TIFF.open(fname)\n",
        "    image = tif.read_image()\n",
        "\n",
        "    # Padding as required and cropping\n",
        "    crops_list = crops(image)\n",
        "    #print(len(crops_list))\n",
        "    trainx_list = trainx_list + crops_list\n",
        "\n",
        "# Array of all the cropped Training sat Images\n",
        "trainx = np.asarray(trainx_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc-pRTFX5Fcu"
      },
      "source": [
        "#### Reading, padding, cropping and making array of all the cropped images of all the trainig gt images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt1efgYr9CJs",
        "outputId": "d7929865-c55d-4d53-e011-afade48ab41a"
      },
      "source": [
        "#print(libtiff.__version__)\n",
        "trainy_list = []\n",
        "\n",
        "for fname in filelist_trainy[:13]:\n",
        "\n",
        "    # Reading the image\n",
        "    print(fname)\n",
        "    image = TIFF.open(fname).read_image()\n",
        "    #image = tif.read_image()\n",
        "\n",
        "    # Padding as required and cropping\n",
        "    crops_list =crops(image)\n",
        "\n",
        "    trainy_list = trainy_list + crops_list\n",
        "\n",
        "# Array of all the cropped Training gt Images\n",
        "trainy = np.asarray(trainy_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dataset/gt/1.tif\n",
            "/content/drive/My Drive/dataset/gt/2.tif\n",
            "/content/drive/My Drive/dataset/gt/3.tif\n",
            "/content/drive/My Drive/dataset/gt/4.tif\n",
            "/content/drive/My Drive/dataset/gt/5.tif\n",
            "/content/drive/My Drive/dataset/gt/6.tif\n",
            "/content/drive/My Drive/dataset/gt/7.tif\n",
            "/content/drive/My Drive/dataset/gt/8.tif\n",
            "/content/drive/My Drive/dataset/gt/9.tif\n",
            "/content/drive/My Drive/dataset/gt/10.tif\n",
            "/content/drive/My Drive/dataset/gt/11.tif\n",
            "/content/drive/My Drive/dataset/gt/12.tif\n",
            "/content/drive/My Drive/dataset/gt/13.tif\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT9VyH0m5KB9"
      },
      "source": [
        "#### Reading, padding, cropping and making array of all the cropped images of all the testing sat images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8w8JElL9K9l"
      },
      "source": [
        "testx_list = []\n",
        "\n",
        "#for fname in filelist_trainx[3]:\n",
        "\n",
        "    # Reading the image\n",
        "tif = TIFF.open(filelist_trainx[13])\n",
        "image = tif.read_image()\n",
        "\n",
        "# Padding as required and cropping\n",
        "crops_list = crops(image)\n",
        "\n",
        "testx_list = testx_list + crops_list\n",
        "\n",
        "# Array of all the cropped Testing sat Images\n",
        "testx = np.asarray(testx_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67rVUZRQ5MHg"
      },
      "source": [
        "#### Reading, padding, cropping and making array of all the cropped images of all the testing sat images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g03Mifi99QeT"
      },
      "source": [
        "testy_list = []\n",
        "\n",
        "# Reading the image\n",
        "tif = TIFF.open(filelist_trainy[13])\n",
        "image = tif.read_image()\n",
        "# Padding as required and cropping\n",
        "crops_list = crops(image)\n",
        "\n",
        "testy_list = testy_list + crops_list\n",
        "\n",
        "# Array of all the cropped Testing sat Images\n",
        "testy = np.asarray(testy_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjWnr0K_5PRg"
      },
      "source": [
        "#### Making array of all the training sat images as it is without any cropping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quZSv_6L9UAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "042fbd33-4031-4f54-f921-4165e2982ae8"
      },
      "source": [
        "xtrain_list = []\n",
        "\n",
        "for fname in filelist_trainx:\n",
        "\n",
        "    # Reading the image\n",
        "    tif = TIFF.open(fname)\n",
        "    image = tif.read_image()\n",
        "\n",
        "    crop_size = 128\n",
        "\n",
        "    stride = 64\n",
        "\n",
        "    h, w, c = image.shape\n",
        "\n",
        "    n_h = int(int(h/stride))\n",
        "    n_w = int(int(w/stride))\n",
        "\n",
        "\n",
        "    image = padding(image, w, h, c, crop_size, stride, n_h, n_w)\n",
        "\n",
        "    xtrain_list.append(image)\n",
        "\n",
        "x_train = np.asarray(xtrain_list)\n",
        "tif = TIFF.open('/content/drive/My Drive/dataset/sat/4.tif')\n",
        "image = tif.read_image()\n",
        "crop_size = 128\n",
        "\n",
        "stride = 64\n",
        "h, w, c = image.shape\n",
        "\n",
        "n_h = int(int(h/stride))\n",
        "n_w = int(int(w/stride))\n",
        "\n",
        "\n",
        "image = padding(image, w, h, c, crop_size, stride, n_h, n_w)\n",
        "x_train = image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxH-1xKy5RxZ"
      },
      "source": [
        "#### Making array of all the training gt images as it is without any cropping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyT0BjE89Wad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8064e64-cfd2-4a99-8fb5-6badca4c0ae5"
      },
      "source": [
        "ytrain_list = []\n",
        "\n",
        "for fname in filelist_trainy:\n",
        "\n",
        "    # Reading the image\n",
        "    tif = TIFF.open(fname)\n",
        "    image = tif.read_image()\n",
        "\n",
        "    crop_size = 128\n",
        "\n",
        "    stride = 64\n",
        "\n",
        "    h, w, c = image.shape\n",
        "\n",
        "    n_h = int(int(h/stride))\n",
        "    n_w = int(int(w/stride))\n",
        "\n",
        "\n",
        "    image = padding(image, w, h, c, crop_size, stride, n_h, n_w)\n",
        "\n",
        "    ytrain_list.append(image)\n",
        "\n",
        "y_train = np.asarray(ytrain_list)\n",
        "\n",
        "\n",
        "tif = TIFF.open('/content/drive/My Drive/dataset/gt/4.tif')\n",
        "image = tif.read_image()\n",
        "crop_size = 128\n",
        "\n",
        "stride = 64\n",
        "\n",
        "h, w, c = image.shape\n",
        "\n",
        "n_h = int(int(h/stride))\n",
        "n_w = int(int(w/stride))\n",
        "\n",
        "\n",
        "image = padding(image, w, h, c, crop_size, stride, n_h, n_w)\n",
        "y_train = image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOyDkPxP5UNQ"
      },
      "source": [
        "#One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlmcr0eN9bxK"
      },
      "source": [
        "color_dict = {0: (0, 0, 0), #black=road\n",
        "              1: (0, 125, 0), #dark green=tree\n",
        "              2: (150, 80, 0), #brown= soil\n",
        "              3: (255, 255, 0), #yellow=rail\n",
        "              4: (100, 100, 100), #grey=buildings\n",
        "              5: (0, 255, 0), #green= field\n",
        "              6: (0, 0, 150), #blue=water\n",
        "              7: (150, 150, 255), #purple= swimming pool\n",
        "              8: (255, 255, 255)} #white=unclassified\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyUSlaHt9e1E"
      },
      "source": [
        "def rgb_to_onehot(rgb_arr, color_dict):\n",
        "    num_classes = len(color_dict)\n",
        "    shape = rgb_arr.shape[:2]+(num_classes,)\n",
        "    #print(shape)\n",
        "    arr = np.zeros( shape, dtype=np.int8 )\n",
        "    for i, cls in enumerate(color_dict):\n",
        "        arr[:,:,i] = np.all(rgb_arr.reshape( (-1,3) ) == color_dict[i], axis=1).reshape(shape[:2])\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbmStbdGuHeK"
      },
      "source": [
        "def onehot_to_rgb(onehot, color_dict):\n",
        "    single_layer = np.argmax(onehot, axis=-1)\n",
        "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
        "    for k in color_dict.keys():\n",
        "        output[single_layer==k] = color_dict[k]\n",
        "    return np.uint8(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1JMTQ1FuKUv"
      },
      "source": [
        "# Convert trainy and testy into one hot encode\n",
        "\n",
        "trainy_hot = []\n",
        "\n",
        "for i in range(trainy.shape[0]):\n",
        "\n",
        "    hot_img = rgb_to_onehot(trainy[i], color_dict)\n",
        "\n",
        "    trainy_hot.append(hot_img)\n",
        "\n",
        "trainy_hot = np.asarray(trainy_hot)\n",
        "\n",
        "testy_hot = []\n",
        "\n",
        "for i in range(testy.shape[0]):\n",
        "\n",
        "    hot_img = rgb_to_onehot(testy[i], color_dict)\n",
        "\n",
        "    testy_hot.append(hot_img)\n",
        "\n",
        "testy_hot = np.asarray(testy_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxTcNKhkubtF"
      },
      "source": [
        "#U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOevhPKetj9Z"
      },
      "source": [
        "def unet(shape = (None,None,4)):\n",
        "\n",
        "    # Left side of the U-Net\n",
        "    inputs = Input(shape)\n",
        "#    in_shape = inputs.shape\n",
        "#    print(in_shape)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    # Bottom of the U-Net\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    # Upsampling Starts, right side of the U-Net\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv9)\n",
        "    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'random_normal')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "\n",
        "    # Output layer of the U-Net with a softmax activation\n",
        "    conv10 = Conv2D(9, 1, activation = 'softmax')(conv9)\n",
        "\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = 0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    #filelist_modelweights = sorted(glob.glob('*.h5'), key=numericalSort)\n",
        "\n",
        "    #if 'model_nocropping.h5' in filelist_modelweights:\n",
        "     #   model.load_weights('model_nocropping.h5')\n",
        "    ##model.load_weights(\"model_onehot.h5\")\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWcWN5o-0ho8",
        "outputId": "edf9be8c-e910-421f-d460-db3bb0e15de5"
      },
      "source": [
        "model = unet()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, None, None, 6 2368        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 6 36928       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, None, 6 256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, None, None, 6 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 1 73856       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 1 147584      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 1 512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 2 295168      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 2 590080      conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 2 1024        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 2 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 5 1180160     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 5 2359808     conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 5 2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, None, 5 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 5 0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, None, None, 1 4719616     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, None, None, 1 9438208     conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 1 4096        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, None, None, 1 0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, None, None, 5 2097664     up_sampling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, None, 1 0           dropout[0][0]                    \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, None, None, 5 4719104     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, None, None, 5 2359808     conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 5 2048        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, None, None, 5 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, None, None, 2 524544      up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 5 0           batch_normalization_2[0][0]      \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, None, None, 2 1179904     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, None, None, 2 590080      conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 2 1024        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, None, None, 2 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, None, None, 1 131200      up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, None, 2 0           batch_normalization_1[0][0]      \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, None, None, 1 295040      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, None, None, 1 147584      conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 1 512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, None, None, 6 32832       up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, None, None, 1 0           batch_normalization[0][0]        \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, None, None, 6 73792       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, None, None, 6 36928       conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, None, None, 1 9232        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, None, 1 64          conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, None, None, 9 153         batch_normalization_8[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 31,053,225\n",
            "Trainable params: 31,047,433\n",
            "Non-trainable params: 5,792\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvjrlsmH5eL-"
      },
      "source": [
        "#Training and testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNWdw-pcuOeZ",
        "outputId": "898d141a-fbfd-4fe7-e3e1-a17ef1bafc4c"
      },
      "source": [
        "history = model.fit(trainx, trainy_hot, epochs=20, validation_data = (testx, testy_hot),batch_size=64, verbose=1)\n",
        "model.save(\"model_onehot.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "249/249 [==============================] - 647s 2s/step - loss: 1.4644 - accuracy: 0.5412 - val_loss: 1.1385 - val_accuracy: 0.7083\n",
            "Epoch 2/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.8087 - accuracy: 0.7791 - val_loss: 0.6838 - val_accuracy: 0.7966\n",
            "Epoch 3/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.6538 - accuracy: 0.8126 - val_loss: 0.6711 - val_accuracy: 0.7942\n",
            "Epoch 4/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.5420 - accuracy: 0.8378 - val_loss: 0.7150 - val_accuracy: 0.7920\n",
            "Epoch 5/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.4715 - accuracy: 0.8524 - val_loss: 0.6214 - val_accuracy: 0.8058\n",
            "Epoch 6/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.4226 - accuracy: 0.8620 - val_loss: 0.6843 - val_accuracy: 0.8032\n",
            "Epoch 7/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.3759 - accuracy: 0.8737 - val_loss: 0.5890 - val_accuracy: 0.8105\n",
            "Epoch 8/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.3494 - accuracy: 0.8794 - val_loss: 0.6991 - val_accuracy: 0.8014\n",
            "Epoch 9/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.3165 - accuracy: 0.8883 - val_loss: 0.7183 - val_accuracy: 0.7969\n",
            "Epoch 10/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.2927 - accuracy: 0.8947 - val_loss: 0.5869 - val_accuracy: 0.8192\n",
            "Epoch 11/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.2736 - accuracy: 0.9003 - val_loss: 0.5789 - val_accuracy: 0.8176\n",
            "Epoch 12/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.2572 - accuracy: 0.9047 - val_loss: 0.7098 - val_accuracy: 0.8033\n",
            "Epoch 13/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.2381 - accuracy: 0.9111 - val_loss: 0.6566 - val_accuracy: 0.8098\n",
            "Epoch 14/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.2240 - accuracy: 0.9154 - val_loss: 0.6185 - val_accuracy: 0.8177\n",
            "Epoch 15/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.2118 - accuracy: 0.9193 - val_loss: 0.6224 - val_accuracy: 0.8149\n",
            "Epoch 16/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.1992 - accuracy: 0.9237 - val_loss: 0.7212 - val_accuracy: 0.8083\n",
            "Epoch 17/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.1860 - accuracy: 0.9284 - val_loss: 0.6481 - val_accuracy: 0.8184\n",
            "Epoch 18/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.1794 - accuracy: 0.9305 - val_loss: 0.6554 - val_accuracy: 0.8164\n",
            "Epoch 19/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.1678 - accuracy: 0.9347 - val_loss: 0.6583 - val_accuracy: 0.8153\n",
            "Epoch 20/20\n",
            "249/249 [==============================] - 564s 2s/step - loss: 0.1599 - accuracy: 0.9375 - val_loss: 0.6707 - val_accuracy: 0.8197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUw7UMI7WIiC"
      },
      "source": [
        "####Accuracy and Loss plots for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "5Yd98ei8uS6R",
        "outputId": "40df7bca-1160-437b-8b86-a9982a5f02f6"
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('acc_plot.png')\n",
        "plt.show()\n",
        "plt.close()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.savefig('loss_plot.png')\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QKtfx-koCA8"
      },
      "source": [
        "# Slicing the image into crop_size*crop_size crops with a stride of crop_size/2 and makking list out of them\n",
        "\n",
        "def crops(a, crop_size = 128):\n",
        "\n",
        "    stride = 32\n",
        "\n",
        "    croped_images = []\n",
        "    h, w, c = a.shape\n",
        "\n",
        "    n_h = int(int(h/stride))\n",
        "    n_w = int(int(w/stride))\n",
        "\n",
        "    # Padding using the padding function we wrote\n",
        "    ##a = padding(a, w, h, c, crop_size, stride, n_h, n_w)\n",
        "\n",
        "    # Resizing as required\n",
        "    ##a = resize(a, stride, n_h, n_w)\n",
        "\n",
        "    # Adding pixals as required\n",
        "    a = add_pixals(a, h, w, c, n_h, n_w, crop_size, stride)\n",
        "\n",
        "    # Slicing the image into 128*128 crops with a stride of 64\n",
        "    for i in range(n_h-1):\n",
        "        for j in range(n_w-1):\n",
        "            crop_x = a[(i*stride):((i*stride)+crop_size), (j*stride):((j*stride)+crop_size), :]\n",
        "            croped_images.append(crop_x)\n",
        "    return croped_images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebp6RNr56KPa"
      },
      "source": [
        "#### Making array of all the training sat images as it is without any cropping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOqU_Aalo2Cl"
      },
      "source": [
        "xtrain_list = []\n",
        "\n",
        "for fname in filelist_trainx[:13]:\n",
        "\n",
        "    # Reading the image\n",
        "    tif = TIFF.open(fname)\n",
        "    image = tif.read_image()\n",
        "\n",
        "    crop_size = 128\n",
        "\n",
        "    stride = 32\n",
        "\n",
        "    h, w, c = image.shape\n",
        "\n",
        "    n_h = int(int(h/stride))\n",
        "    n_w = int(int(w/stride))\n",
        "\n",
        "\n",
        "    image = padding(image, w, h, c, crop_size, stride, n_h, n_w)\n",
        "\n",
        "    xtrain_list.append(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR0vkUHW6NNc"
      },
      "source": [
        "#### Making array of all the training gt images as it is without any cropping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZFU6yP3o74D"
      },
      "source": [
        "ytrain_list = []\n",
        "\n",
        "for fname in filelist_trainy[:13]:\n",
        "\n",
        "    # Reading the image\n",
        "    tif = TIFF.open(fname)\n",
        "    image = tif.read_image()\n",
        "\n",
        "    crop_size = 128\n",
        "\n",
        "    stride = 32\n",
        "\n",
        "    h, w, c = image.shape\n",
        "\n",
        "    n_h = int(int(h/stride))\n",
        "    n_w = int(int(w/stride))\n",
        "\n",
        "\n",
        "    image = padding(image, w, h, c, crop_size, stride, n_h, n_w)\n",
        "\n",
        "    ytrain_list.append(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6u02pvAo8xI",
        "outputId": "25bc1992-fd8a-4d2a-d276-7d865ec9bee5"
      },
      "source": [
        "y_train = np.asarray(ytrain_list)\n",
        "x_train = np.asarray(xtrain_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5UyrHfJ6R7l"
      },
      "source": [
        "#### Making array of all the training sat images as it is without any cropping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0jwsWkPo_Mh"
      },
      "source": [
        "# Reading the image\n",
        "tif = TIFF.open(filelist_trainx[13])\n",
        "image = tif.read_image()\n",
        "\n",
        "crop_size = 128\n",
        "\n",
        "stride = 32\n",
        "\n",
        "h, w, c = image.shape\n",
        "\n",
        "n_h = int(int(h/stride))\n",
        "n_w = int(int(w/stride))\n",
        "\n",
        "image = add_pixals(image, h, w, c, n_h, n_w, crop_size, stride)\n",
        "\n",
        "#x_val = np.reshape(image, (1,h,w,c))\n",
        "x_val = image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMBSj73H6WOl"
      },
      "source": [
        "#### Making array of all the training gt images as it is without any cropping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5MS6caypFAh"
      },
      "source": [
        "# Reading the image\n",
        "tif = TIFF.open(filelist_trainy[13])\n",
        "image = tif.read_image()\n",
        "\n",
        "crop_size = 128\n",
        "\n",
        "stride = 32\n",
        "\n",
        "h, w, c = image.shape\n",
        "\n",
        "n_h = int(int(h/stride))\n",
        "n_w = int(int(w/stride))\n",
        "\n",
        "\n",
        "image = add_pixals(image, h, w, c, n_h, n_w, crop_size, stride)\n",
        "\n",
        "#y_val1 = np.reshape(image, (1,h,w,c))\n",
        "y_val = image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS7fceK2pJvg"
      },
      "source": [
        "xtest_list1 = []\n",
        "\n",
        "for fname in filelist_testx:\n",
        "\n",
        "    # Reading the image\n",
        "    tif = TIFF.open(fname)\n",
        "    image = tif.read_image()\n",
        "\n",
        "    crop_size = 128\n",
        "\n",
        "    stride = 32\n",
        "\n",
        "    h, w, c = image.shape\n",
        "\n",
        "    n_h = int(int(h/stride))\n",
        "    n_w = int(int(w/stride))\n",
        "\n",
        "\n",
        "    image = add_pixals(image, h, w, c, n_h, n_w, crop_size, stride)\n",
        "\n",
        "    xtest_list1.append(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XraAdXZ6pMbP"
      },
      "source": [
        "def testing(model, trainx, trainy, testx, testy, weights_file = \"model_onehot.h5\"):\n",
        "\n",
        "    pred_train_all = []\n",
        "    pred_val_all = []\n",
        "\n",
        "    model.load_weights(weights_file)\n",
        "\n",
        "    Y_pred_train = model.predict(trainx)\n",
        "\n",
        "    for k in range(Y_pred_train.shape[0]):\n",
        "\n",
        "        pred_train_all.append(Y_pred_train[k])\n",
        "\n",
        "    Y_gt_train = [rgb_to_onehot(arr, color_dict) for arr in trainy]\n",
        "\n",
        "    Y_pred_val = model.predict(testx)\n",
        "\n",
        "    for k in range(Y_pred_val.shape[0]):\n",
        "\n",
        "        pred_val_all.append(Y_pred_val[k])\n",
        "\n",
        "    Y_gt_val = [rgb_to_onehot(arr, color_dict) for arr in testy]\n",
        "\n",
        "    return pred_train_all, Y_gt_train, pred_val_all, Y_gt_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS6hDS4xpvZI"
      },
      "source": [
        "def testing_diffsizes(model, trainx, trainy, testx, testy, weights_file = \"model_augment.h5\"):\n",
        "\n",
        "    pred_train_all = []\n",
        "    pred_test_all = []\n",
        "\n",
        "\n",
        "    model.load_weights(weights_file)\n",
        "\n",
        "    for i in range(len(trainx)):\n",
        "        img = trainx[i]\n",
        "        h,w,c = img.shape\n",
        "        img = np.reshape(img, (1,h,w,c))\n",
        "        Y_pred_train = model.predict(img)\n",
        "        bb,h,w,c = Y_pred_train.shape\n",
        "        Y_pred_train = np.reshape(Y_pred_train, (h,w,c))\n",
        "        pred_train_all.append(Y_pred_train)\n",
        "\n",
        "#    for k in range(Y_pred_train.shape[0]):\n",
        "\n",
        "#        pred_train_all.append(Y_pred_train[k])\n",
        "\n",
        "    Y_gt_train = [rgb_to_onehot(arr, color_dict) for arr in trainy]\n",
        "\n",
        "    img = testx\n",
        "    h,w,c = img.shape\n",
        "    img = np.reshape(img, (1,h,w,c))\n",
        "    Y_pred_test = model.predict(img)\n",
        "    bb,h,w,c = Y_pred_test.shape\n",
        "    Y_pred_test = np.reshape(Y_pred_test, (h,w,c))\n",
        "    pred_test_all.append(Y_pred_test)\n",
        "\n",
        "#    for k in range(Y_pred_val.shape[0]):\n",
        "\n",
        "#        pred_test_all.append(Y_pred_val[k])\n",
        "\n",
        "    Y_gt_val = [rgb_to_onehot(testy, color_dict)]\n",
        "\n",
        "    return pred_train_all, Y_gt_train, pred_test_all, Y_gt_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW3h_vTHpxPV",
        "outputId": "0bc3fb36-b5ff-4c0a-dbd1-acb648eb80f0"
      },
      "source": [
        "##pred_train_all, Y_gt_train, pred_val_all, Y_gt_val = testing(model, trainx, trainy, testx, testy, weights_file = \"model_onehot.h5\")\n",
        "\n",
        "pred_train_3, Y_gt_train_3, pred_val_all, Y_gt_val = testing_diffsizes(model, x_train, y_train, x_val, y_val, weights_file = \"model_onehot.h5\")\n",
        "\n",
        "print(pred_val_all[0].shape)\n",
        "print(Y_gt_val[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(704, 864, 9)\n",
            "(704, 864, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxNllkGap11q"
      },
      "source": [
        "# Convert onehot to label\n",
        "def to_class_no(y_hot_list):\n",
        "    y_class_list = []\n",
        "\n",
        "    n = len(y_hot_list)\n",
        "\n",
        "    for i in range(n):\n",
        "\n",
        "        out = np.argmax(y_hot_list[i])\n",
        "\n",
        "        y_class_list.append(out)\n",
        "\n",
        "    return y_class_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIKjvPOV6tVO"
      },
      "source": [
        "#Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsJU6nLkqAYH"
      },
      "source": [
        "def conf_matrix(Y_gt, Y_pred, num_classes = 9):\n",
        "\n",
        "    total_pixels = 0\n",
        "    kappa_sum = 0\n",
        "    sudo_confusion_matrix = np.zeros((num_classes, num_classes))\n",
        "\n",
        "#    if len(Y_pred.shape) == 3:\n",
        "#        h,w,c = Y_pred.shape\n",
        "#        Y_pred = np.reshape(Y_pred, (1,))\n",
        "\n",
        "    n = len(Y_pred)\n",
        "\n",
        "    for i in range(n):\n",
        "        y_pred = Y_pred[i]\n",
        "        y_gt = Y_gt[i]\n",
        "\n",
        "        #y_pred_hotcode = hotcode(y_pred)\n",
        "        #y_gt_hotcode = hotcode(y_gt)\n",
        "\n",
        "        pred = np.reshape(y_pred, (y_pred.shape[0]*y_pred.shape[1], y_pred.shape[2]))\n",
        "        gt = np.reshape(y_gt, (y_gt.shape[0]*y_gt.shape[1], y_gt.shape[2]))\n",
        "\n",
        "        pred = [i for i in pred]\n",
        "        gt = [i for i in gt]\n",
        "\n",
        "        pred = to_class_no(pred)\n",
        "        gt = to_class_no(gt)\n",
        "\n",
        "#        pred.tolist()\n",
        "#        gt.tolist()\n",
        "\n",
        "        gt = np.asarray(gt, dtype = 'int32')\n",
        "        pred = np.asarray(pred, dtype = 'int32')\n",
        "\n",
        "        conf_matrix = confusion_matrix(gt, pred, labels=[0,1,2,3,4,5,6,7,8])\n",
        "\n",
        "        kappa = cohen_kappa_score(gt,pred, labels=[0,1,2,3,4,5,6,7])\n",
        "\n",
        "        pixels = len(pred)\n",
        "        total_pixels = total_pixels+pixels\n",
        "\n",
        "        sudo_confusion_matrix = sudo_confusion_matrix + conf_matrix\n",
        "\n",
        "        kappa_sum = kappa_sum + kappa\n",
        "\n",
        "    final_confusion_matrix = sudo_confusion_matrix\n",
        "\n",
        "    final_kappa = kappa_sum/n\n",
        "\n",
        "    return final_confusion_matrix, final_kappa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE5uPdakqLr-",
        "outputId": "d8f758cc-39ce-4bf9-b7a0-d170a632d554"
      },
      "source": [
        "confusion_matrix_train, kappa_train = conf_matrix(Y_gt_train_3, pred_train_3, num_classes = 9)\n",
        "print('Confusion Matrix for training')\n",
        "print(confusion_matrix_train)\n",
        "print('Kappa Coeff for training without unclassified pixels')\n",
        "print(kappa_train)\n",
        "\n",
        "confusion_matrix_test, kappa_test = conf_matrix(Y_gt_val, pred_val_all, num_classes = 9)\n",
        "print('Confusion Matrix for validation')\n",
        "print(confusion_matrix_test)\n",
        "print('Kappa Coeff for validation without unclassified pixels')\n",
        "print(kappa_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for training\n",
            "[[5.067813e+06 2.931000e+03 4.290000e+02 7.800000e+02 1.032900e+04\n",
            "  9.010000e+02 1.900000e+02 0.000000e+00 4.327000e+04]\n",
            " [1.947000e+03 1.475129e+06 1.900000e+01 9.600000e+01 5.780000e+02\n",
            "  1.229000e+03 5.500000e+01 0.000000e+00 3.692200e+04]\n",
            " [1.680000e+02 4.030000e+02 1.654930e+05 9.000000e+00 4.700000e+01\n",
            "  9.500000e+01 0.000000e+00 0.000000e+00 3.069000e+03]\n",
            " [2.500000e+01 2.200000e+01 0.000000e+00 1.796070e+05 2.620000e+02\n",
            "  0.000000e+00 0.000000e+00 0.000000e+00 1.695000e+03]\n",
            " [9.218000e+03 9.640000e+02 2.200000e+01 2.180000e+02 3.762205e+06\n",
            "  1.620000e+02 7.000000e+00 0.000000e+00 8.151200e+04]\n",
            " [5.830000e+02 8.337000e+03 1.690000e+02 2.300000e+01 5.440000e+02\n",
            "  1.195257e+06 0.000000e+00 0.000000e+00 3.798800e+04]\n",
            " [3.380000e+02 2.320000e+02 0.000000e+00 0.000000e+00 4.000000e+01\n",
            "  0.000000e+00 8.889200e+05 0.000000e+00 6.170000e+03]\n",
            " [0.000000e+00 2.100000e+01 0.000000e+00 0.000000e+00 1.000000e+00\n",
            "  2.000000e+00 0.000000e+00 2.654900e+04 9.000000e+02]\n",
            " [2.522050e+05 2.625680e+05 4.301000e+03 1.065400e+04 2.205860e+05\n",
            "  4.694900e+04 1.027100e+04 7.180000e+02 6.288189e+06]]\n",
            "Kappa Coeff for training without unclassified pixels\n",
            "0.9954356531492983\n",
            "Confusion Matrix for validation\n",
            "[[7.63770e+04 1.73000e+02 0.00000e+00 0.00000e+00 5.35300e+03 0.00000e+00\n",
            "  2.50000e+01 0.00000e+00 1.86150e+04]\n",
            " [1.75000e+02 2.82620e+04 0.00000e+00 0.00000e+00 8.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 4.10900e+03]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [2.11400e+03 1.49000e+02 0.00000e+00 0.00000e+00 1.22044e+05 0.00000e+00\n",
            "  7.00000e+00 0.00000e+00 8.82100e+03]\n",
            " [7.00000e+00 1.39200e+03 0.00000e+00 0.00000e+00 9.00000e+00 1.50800e+03\n",
            "  0.00000e+00 0.00000e+00 3.53500e+03]\n",
            " [2.60000e+01 3.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  1.98982e+05 0.00000e+00 2.93800e+03]\n",
            " [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00]\n",
            " [1.40730e+04 6.25300e+03 0.00000e+00 0.00000e+00 2.91030e+04 6.30000e+01\n",
            "  6.99000e+03 0.00000e+00 7.71420e+04]]\n",
            "Kappa Coeff for validation without unclassified pixels\n",
            "0.967766506636609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCDfa52sSAzl"
      },
      "source": [
        "#Particular class accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_hS3Ewb6znz"
      },
      "source": [
        "#### Pass Confusion matrix, label to which the accuracy needs to be found, number of classes to be considered\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjkdIryTqMTl"
      },
      "source": [
        "def acc_of_class(class_label, conf_matrix, num_classes = 8):\n",
        "\n",
        "    numerator = conf_matrix[class_label, class_label]\n",
        "\n",
        "    denorminator = 0\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        denorminator = denorminator + conf_matrix[class_label, i]\n",
        "\n",
        "    acc_of_class = numerator/denorminator\n",
        "\n",
        "    return acc_of_class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnSUPyx8twG0"
      },
      "source": [
        "## On training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUjRwsztyBb"
      },
      "source": [
        "#### Find accuray of all the classes NOT considering the unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGfXTgb2qQCY",
        "outputId": "21b2e6d7-103f-4772-a84f-2c4fa8ce32ba"
      },
      "source": [
        "for i in range(8):\n",
        "    acc_of_cl = acc_of_class(class_label = i, conf_matrix = confusion_matrix_train, num_classes = 8)\n",
        "    print('Accuracy of class '+str(i) + ' WITHOUT unclassified pixels - Training')\n",
        "    print(acc_of_cl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of class 0 WITHOUT unclassified pixels - Training\n",
            "0.9969390402789644\n",
            "Accuracy of class 1 WITHOUT unclassified pixels - Training\n",
            "0.9973469510558445\n",
            "Accuracy of class 2 WITHOUT unclassified pixels - Training\n",
            "0.9956562283789069\n",
            "Accuracy of class 3 WITHOUT unclassified pixels - Training\n",
            "0.9982825318481958\n",
            "Accuracy of class 4 WITHOUT unclassified pixels - Training\n",
            "0.9971927981263763\n",
            "Accuracy of class 5 WITHOUT unclassified pixels - Training\n",
            "0.991986143397905\n",
            "Accuracy of class 6 WITHOUT unclassified pixels - Training\n",
            "0.9993142446010814\n",
            "Accuracy of class 7 WITHOUT unclassified pixels - Training\n",
            "0.9990968276069695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqwSyOn1tsNc"
      },
      "source": [
        "#### Find accuray of all the classes considering the unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Q7VHEhqWxz",
        "outputId": "1debeaab-5df4-40c6-8cb6-0964d95bb230"
      },
      "source": [
        "for i in range(9):\n",
        "    acc_of_cl = acc_of_class(class_label = i, conf_matrix = confusion_matrix_train, num_classes = 9)\n",
        "    print('Accuracy of class '+str(i) + ' WITH unclassified pixels - Training')\n",
        "    print(acc_of_cl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of class 0 WITH unclassified pixels - Training\n",
            "0.9885246544376115\n",
            "Accuracy of class 1 WITH unclassified pixels - Training\n",
            "0.9730562839096951\n",
            "Accuracy of class 2 WITH unclassified pixels - Training\n",
            "0.9776056803950758\n",
            "Accuracy of class 3 WITH unclassified pixels - Training\n",
            "0.9889654261030444\n",
            "Accuracy of class 4 WITH unclassified pixels - Training\n",
            "0.9761038816825225\n",
            "Accuracy of class 5 WITH unclassified pixels - Training\n",
            "0.9616670997931452\n",
            "Accuracy of class 6 WITH unclassified pixels - Training\n",
            "0.992430501283912\n",
            "Accuracy of class 7 WITH unclassified pixels - Training\n",
            "0.96636697848797\n",
            "Accuracy of class 8 WITH unclassified pixels - Training\n",
            "0.8861045980654246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6hZo6YOtiTB"
      },
      "source": [
        "## On validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wajd_szztkUY"
      },
      "source": [
        "#### Find accuray of all the classes NOT considering the unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pPcXNB7qYtl",
        "outputId": "b624e6ca-5015-4c5e-f24f-840c669ef52a"
      },
      "source": [
        "for i in range(8):\n",
        "    acc_of_cl = acc_of_class(class_label = i, conf_matrix = confusion_matrix_test, num_classes = 8)\n",
        "    print('Accuracy of class '+str(i) + ' WITHOUT unclassified pixels - Validation')\n",
        "    print(acc_of_cl)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of class 0 WITHOUT unclassified pixels - Validation\n",
            "0.9322453861927547\n",
            "Accuracy of class 1 WITHOUT unclassified pixels - Validation\n",
            "0.9935665319036737\n",
            "Accuracy of class 2 WITHOUT unclassified pixels - Validation\n",
            "nan\n",
            "Accuracy of class 3 WITHOUT unclassified pixels - Validation\n",
            "nan\n",
            "Accuracy of class 4 WITHOUT unclassified pixels - Validation\n",
            "0.9817397879563042\n",
            "Accuracy of class 5 WITHOUT unclassified pixels - Validation\n",
            "0.5171467764060357\n",
            "Accuracy of class 6 WITHOUT unclassified pixels - Validation\n",
            "0.9998542794116908\n",
            "Accuracy of class 7 WITHOUT unclassified pixels - Validation\n",
            "nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQhN48ExteXZ"
      },
      "source": [
        "#### Find accuray of all the classes considering the unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx1GJXbkqaTV",
        "outputId": "2198abd3-5990-418f-8bec-9f2da2938811"
      },
      "source": [
        "for i in range(9):\n",
        "    acc_of_cl = acc_of_class(class_label = i, conf_matrix = confusion_matrix_test, num_classes = 9)\n",
        "    print('Accuracy of class '+str(i) + ' WITH unclassified pixels - Validation')\n",
        "    print(acc_of_cl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of class 0 WITH unclassified pixels - Validation\n",
            "0.7596451269606039\n",
            "Accuracy of class 1 WITH unclassified pixels - Validation\n",
            "0.8681575228850525\n",
            "Accuracy of class 2 WITH unclassified pixels - Validation\n",
            "nan\n",
            "Accuracy of class 3 WITH unclassified pixels - Validation\n",
            "nan\n",
            "Accuracy of class 4 WITH unclassified pixels - Validation\n",
            "0.9166935817027829\n",
            "Accuracy of class 5 WITH unclassified pixels - Validation\n",
            "0.2337622074097039\n",
            "Accuracy of class 6 WITH unclassified pixels - Validation\n",
            "0.9853081718651738\n",
            "Accuracy of class 7 WITH unclassified pixels - Validation\n",
            "nan\n",
            "Accuracy of class 8 WITH unclassified pixels - Validation\n",
            "0.577306471891277\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V23rg7R9SJ2C"
      },
      "source": [
        "#Overall accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXSgrqiYtX8m"
      },
      "source": [
        "### Calulating over all accuracy with and without unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnUPDCsbqcx1"
      },
      "source": [
        "def overall_acc(conf_matrix, include_unclassified_pixels = False):\n",
        "\n",
        "    if include_unclassified_pixels:\n",
        "\n",
        "        numerator = 0\n",
        "        for i in range(9):\n",
        "\n",
        "            numerator = numerator + conf_matrix[i,i]\n",
        "\n",
        "        denominator = 0\n",
        "        for i in range(9):\n",
        "            for j in range(9):\n",
        "\n",
        "                denominator = denominator + conf_matrix[i,j]\n",
        "\n",
        "        acc = numerator/denominator\n",
        "\n",
        "        return acc\n",
        "\n",
        "    else:\n",
        "\n",
        "        numerator = 0\n",
        "        for i in range(8):\n",
        "\n",
        "            numerator = numerator + conf_matrix[i,i]\n",
        "\n",
        "        denominator = 0\n",
        "        for i in range(8):\n",
        "            for j in range(8):\n",
        "\n",
        "                denominator = denominator + conf_matrix[i,j]\n",
        "\n",
        "        acc = numerator/denominator\n",
        "\n",
        "        return acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9J1tE1LtP4O"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTYZ2TxXtRwy"
      },
      "source": [
        "#### Over all accuracy without unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJt2VwxfqfPy",
        "outputId": "36e2fa34-da37-4b8f-fa91-aeceb3baad30"
      },
      "source": [
        "print('Over all accuracy WITHOUT unclassified pixels - Training')\n",
        "print(overall_acc(conf_matrix = confusion_matrix_train, include_unclassified_pixels = False))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Over all accuracy WITHOUT unclassified pixels - Training\n",
            "0.9967665359434649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik_h1dZgtMdO"
      },
      "source": [
        "#### Over all accuracy with unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbK3LCzeqgwh",
        "outputId": "9d897e1a-362f-4c29-fddd-645424f460b4"
      },
      "source": [
        "print('Over all accuracy WITH unclassified pixels - Training')\n",
        "print(overall_acc(conf_matrix = confusion_matrix_train, include_unclassified_pixels = True))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Over all accuracy WITH unclassified pixels - Training\n",
            "0.9472324082501655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGTGLWBqtD1R"
      },
      "source": [
        "## Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCGnOD96tGZb"
      },
      "source": [
        "\n",
        "#### Over all accuracy without unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QQ6gyjiqjql",
        "outputId": "ab77e64f-5220-48e7-b26b-211c99cc84b1"
      },
      "source": [
        "print('Over all accuracy WITHOUT unclassified pixels - Validation')\n",
        "print(overall_acc(conf_matrix = confusion_matrix_test, include_unclassified_pixels = False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Over all accuracy WITHOUT unclassified pixels - Validation\n",
            "0.9783767813217167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zL3c7UJs6_Q"
      },
      "source": [
        "#### Over all accuracy with unclassified pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obU5m0ouql-K",
        "outputId": "986c0a4f-9912-413e-8de5-a6e8399f1134"
      },
      "source": [
        "print('Over all accuracy WITH unclassified pixels - Validation')\n",
        "print(overall_acc(conf_matrix = confusion_matrix_test, include_unclassified_pixels = True))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Over all accuracy WITH unclassified pixels - Validation\n",
            "0.8291163589015151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SEUeBeSCgu8"
      },
      "source": [
        "#### Convert decimal onehot encode from prediction to actual onehot code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1VwYZqqpA7"
      },
      "source": [
        "def dec_to_onehot(pred_all):\n",
        "\n",
        "    pred_all_onehot_list = []\n",
        "\n",
        "    for img in pred_all:\n",
        "\n",
        "        h, w, c = img.shape\n",
        "\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "\n",
        "                argmax_index = np.argmax(img[i,j])\n",
        "\n",
        "                sudo_onehot_arr = np.zeros((9))\n",
        "\n",
        "                sudo_onehot_arr[argmax_index] = 1\n",
        "\n",
        "                onehot_encode = sudo_onehot_arr\n",
        "\n",
        "                img[i,j,:] = onehot_encode\n",
        "\n",
        "        pred_all_onehot_list.append[img]\n",
        "\n",
        "    return pred_all_onehot_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbGtZqHEsxn5"
      },
      "source": [
        "# Pred on train, val, test and save outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vsqclRJq0ac"
      },
      "source": [
        "weights_file = \"model_onehot.h5\"\n",
        "model.load_weights(weights_file)\n",
        "\n",
        "#y_pred_test_all = []\n",
        "\n",
        "xtrain_list.append(x_val)\n",
        "\n",
        "for i_ in range(len(xtrain_list)):\n",
        "\n",
        "    item = xtrain_list[i_]\n",
        "\n",
        "    h,w,c = item.shape\n",
        "\n",
        "    item = np.reshape(item,(1,h,w,c))\n",
        "\n",
        "    y_pred_train_img = model.predict(item)\n",
        "\n",
        "    ba,h,w,c = y_pred_train_img.shape\n",
        "\n",
        "    y_pred_train_img = np.reshape(y_pred_train_img,(h,w,c))\n",
        "\n",
        "    img = y_pred_train_img\n",
        "    h, w, c = img.shape\n",
        "\n",
        "    for i in range(h):\n",
        "        for j in range(w):\n",
        "\n",
        "            argmax_index = np.argmax(img[i,j])\n",
        "\n",
        "            sudo_onehot_arr = np.zeros((9))\n",
        "\n",
        "            sudo_onehot_arr[argmax_index] = 1\n",
        "\n",
        "            onehot_encode = sudo_onehot_arr\n",
        "\n",
        "            img[i,j,:] = onehot_encode\n",
        "\n",
        "    y_pred_train_img = onehot_to_rgb(img, color_dict)\n",
        "\n",
        "    tif = TIFF.open(filelist_trainx[i_])\n",
        "    image2 = tif.read_image()\n",
        "\n",
        "    h,w,c = image2.shape\n",
        "\n",
        "    y_pred_train_img = y_pred_train_img[:h, :w, :]\n",
        "\n",
        "    imx = Image.fromarray(y_pred_train_img)\n",
        "\n",
        "    imx.save(\"/content/drive/My Drive/dataset/pred/pred\"+str(i_+1)+\".jpg\")\n",
        "\n",
        "\n",
        "\n",
        "for i_ in range(len(xtest_list1)):\n",
        "\n",
        "    item = xtest_list1[i_]\n",
        "\n",
        "    h,w,c = item.shape\n",
        "\n",
        "    item = np.reshape(item,(1,h,w,c))\n",
        "\n",
        "    y_pred_test_img = model.predict(item)\n",
        "\n",
        "    ba,h,w,c = y_pred_test_img.shape\n",
        "\n",
        "    y_pred_test_img = np.reshape(y_pred_test_img,(h,w,c))\n",
        "\n",
        "    img = y_pred_test_img\n",
        "    h, w, c = img.shape\n",
        "\n",
        "    for i in range(h):\n",
        "        for j in range(w):\n",
        "\n",
        "            argmax_index = np.argmax(img[i,j])\n",
        "\n",
        "            sudo_onehot_arr = np.zeros((9))\n",
        "\n",
        "            sudo_onehot_arr[argmax_index] = 1\n",
        "\n",
        "            onehot_encode = sudo_onehot_arr\n",
        "\n",
        "            img[i,j,:] = onehot_encode\n",
        "\n",
        "    y_pred_test_img = onehot_to_rgb(img, color_dict)\n",
        "\n",
        "    tif = TIFF.open(filelist_testx[i_])\n",
        "    image2 = tif.read_image()\n",
        "\n",
        "    h,w,c = image2.shape\n",
        "\n",
        "    y_pred_test_img = y_pred_test_img[:h, :w, :]\n",
        "\n",
        "    imx = Image.fromarray(y_pred_test_img)\n",
        "\n",
        "    imx.save(\"/content/drive/My Drive/dataset/out/out\"+str(i_+1)+\".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}